# Progress Log

Formato por entrada:
- **DATE/TIME**:
- **Paso**:
- **Qué se hizo**:
- **Decisiones**:
- **Riesgos**:
- **Qué queda**:

---

- **DATE/TIME**: 2026-02-27 00:00 UTC
- **Paso**: Paso 1 — creación de roadmap + contratos
- **Qué se hizo**:
  - Se creó roadmap de implementación en prompts incrementales (`docs/ml_roadmap_codex.md`).
  - Se formalizó contrato de arquitectura por capas y puertos (`docs/architecture_contract.md`).
  - Se definió quality gate de core con cobertura >=85% sin UI bloqueante (`docs/ci_quality_gate.md`).
  - Se revisaron docs/README y se añadieron estándares mínimos de ingeniería (`docs/standards.md`) con referencias desde README y TESTING.
- **Decisiones**:
  - Adoptar enfoque Strangler por pasos pequeños.
  - Tratar UI como capa no bloqueante de cobertura en esta fase.
  - Mantener cambios documentales mínimos, sin refactor masivo.
- **Riesgos**:
  - Cobertura actual de core probablemente por debajo del objetivo hasta ejecutar pasos siguientes.
  - Posibles imports cruzados históricos que requieran corrección gradual.
- **Qué queda**:
  - Implementar Prompt 1 del roadmap (automatizar gate en CI real + medición estable).

- **DATE/TIME**: 2026-02-27 00:45 UTC
- **Paso**: Paso 2 — gate + tests core verdes
- **Qué se hizo**:
  - Se ejecutó diagnóstico (`pytest -q` y `pytest -q --maxfail=1 -x`) y se aislaron causas raíz de fallos.
  - Se corrigieron contratos rotos entre capa application/domain/infrastructure en citas (campos/enum/fechas alineados).
  - Se corrigieron mapeos de repositorios de medicamentos/materiales para instanciar modelos de dominio con el nombre de campo canónico.
  - Se corrigió la búsqueda textual en queries de pacientes/médicos/personal (evitando condición AND espuria con teléfono normalizado).
  - Se ajustó test de pacientes para validar el comportamiento real y canónico de `num_historia` autogenerado.
  - Se creó `scripts/quality_gate.py` como comando único reproducible local/CI con gate bloqueante de core >=85% y exclusión de UI.
  - Se actualizó `docs/ci_quality_gate.md` para reflejar implementación real.
- **Decisiones**:
  - Se priorizó arreglo mínimo orientado a estabilidad de CI de core sin refactor masivo.
  - UI queda fuera del gate bloqueante (marker `ui` en pytest, excluido por diseño en el script).
  - No se añadieron herramientas nuevas de lint; solo ejecución condicional si la configuración ya existe.
- **Riesgos**:
  - El cálculo de cobertura usa trazado estándar de Python (no pytest-cov), por restricciones del entorno sin instalación de dependencias.
  - Persisten warnings de adaptador datetime de sqlite3 en Python 3.12 (no bloqueante para este paso).
- **Qué queda**:
  - Cuando haya acceso a dependencias de red/CI, evaluar migración de cálculo de cobertura a `pytest-cov` manteniendo el mismo contrato de core.

- **DATE/TIME**: 2026-02-27 22:05 UTC
- **Paso**: Paso 4: Pipeline extracción citas v1
- **Qué se hizo**:
  - Se definió el contrato de lectura `CitasReadPort` y el read-model `CitaReadModel` para desacoplar la extracción de la capa UI.
  - Se implementó el caso de uso `BuildCitasDataset` para construir filas tabulares con filtros por rango, mapeo tipado e invariantes (`duracion_min >= 0`, control de nulos en notas).
  - Se añadió el adaptador `SqliteCitasReadAdapter` en infraestructura, reutilizando repositorios existentes de citas/incidencias.
  - Se extendió `CitasRepository` con `list_in_range(...)` para lectura temporal canónica.
  - Se añadieron tests unitarios del builder con fakes y un test de contrato del adaptador sin SQLite real.
- **Decisiones**:
  - El puerto vive en `application/ports` porque el caso de uso de extracción pertenece a application y debe depender de abstracciones.
  - El dataset se modela con dataclass (`CitasDatasetRow`) para tipado fuerte y evolución segura hacia features ML.
  - Se reutiliza `IncidenciasRepository.search(cita_id=...)` para derivar `has_incidencias`, evitando duplicación SQL prematura en esta versión v1.
- **Riesgos**:
  - El adaptador actual consulta incidencias por cita (patrón N+1) y puede requerir optimización cuando el volumen crezca.
- **Qué queda**:
  - Añadir transforms de features (p. ej. franja horaria, día semana, lead time).
  - Definir persistencia/versionado de dataset (feature store ligero).
  - Preparar contratos para dataset de entrenamiento/validación.

- **DATE/TIME**: 2026-02-27 22:40 UTC
- **Paso**: Paso 5: Features citas v1 + quality report
- **Qué se hizo**:
  - Se creó el módulo puro de application `application/features/citas_features.py` con DTO canónico `CitasFeatureRow` y transformaciones deterministas desde `list[CitasDatasetRow]`.
  - Se implementó `build_citas_features(...)` con normalización de estado, buckets de duración/notas, señales temporales (`hora_inicio`, `dia_semana`, `is_weekend`) y flag `is_suspicious`.
  - Se implementó `validate_citas_features(...)` para invariantes de calidad mínimas y explícitas.
  - Se implementó `CitasFeatureQualityReport` + `compute_citas_quality_report(...)` con contadores agregados por estado y buckets.
  - Se añadieron tests unitarios dedicados en `tests/test_citas_features.py` para happy path, notas en cero, outliers, validación y reporte.
- **Decisiones**:
  - Outliers de duración (`duracion_min > 240`) no rompen pipeline: se marcan con `is_suspicious=True` y se contabilizan en el reporte.
  - Duración no positiva solo se permite para `estado_norm="cancelada"`; para otros estados la validación lanza error explícito.
  - `missing_count` del reporte se calcula como features con `estado_norm="desconocido"`.
  - `lead_time_min` se omite en v1 porque `CitasDatasetRow` actual no expone `creada_en/reservada_en`.
- **Riesgos**:
  - La normalización de estados cubre alias comunes, pero pueden aparecer variantes nuevas que convenga mapear en una tabla de vocabulario de dominio.
- **Qué queda**:
  - Feature store/versionado de datasets y features.
  - Baseline de entrenamiento/evaluación con split reproducible.
  - Contrato de scoring online/offline y monitoreo de deriva de datos/features.

- **DATE/TIME**: 2026-02-27 23:10 UTC
- **Paso**: Paso 6: Feature Store offline v1
- **Qué se hizo**:
  - Se definió el contrato `FeatureStorePort` en application (contract-first) con operaciones `save`, `load` y `list_versions`.
  - Se implementó `LocalJsonFeatureStore` en infraestructura con persistencia local en `data/feature_store/<dataset>/<version>.json`, creación automática de carpetas y errores explícitos para dataset/versión inexistentes.
  - Se añadió `FeatureStoreService` en application para orquestar el dataset lógico `citas_features`, generar versión timestamp segura para filename y delegar la persistencia/carga al port.
  - Se incorporó test suite dedicada `tests/test_feature_store.py` cubriendo roundtrip, listado de versiones, errores explícitos, no sobreescritura entre versiones y serialización determinista.
- **Decisiones**:
  - Se eligió JSON local frente a SQLite en v1 para minimizar complejidad operativa, facilitar inspección humana y mantener cero dependencia externa.
  - Se fijó serialización determinista (`sort_keys=True`) para asegurar reproducibilidad binaria del artefacto al guardar el mismo contenido.
  - Se mantuvo el servicio de aplicación liviano y sin lógica de infraestructura para respetar Clean Architecture.
- **Riesgos**:
  - JSON completo por versión no escala bien para datasets grandes; podría requerir compresión/particionado.
  - No hay locking ni control de concurrencia en escrituras simultáneas.
  - La validación del payload cargado es mínima (solo tipo lista) y puede ampliarse con esquemas versionados.
- **Qué queda**:
  - Definir online store y estrategia híbrida offline/online.
  - Incorporar locking atómico y manejo robusto de concurrencia.
  - Añadir metadatos de lineage, checksum y gobernanza de versiones.

- **DATE/TIME**: 2026-02-28 00:05 UTC
- **Paso**: Paso 7: Baseline predictor + scoring use case
- **Qué se hizo**:
  - Se definió el contrato de inferencia `PredictorPort` en application con tipos públicos `PredictorInput` y `PredictionResult`.
  - Se implementó `BaselineCitasPredictor` determinista en application (`ml/`) con reglas explicables, score clamped en `[0,1]`, labels (`low/medium/high`) y `reasons` trazables.
  - Se implementó el caso de uso `ScoreCitas` con request/response tipados, carga desde `FeatureStoreService`, soporte de `limit` y mapeo a `ScoredCita`.
  - Se añadieron errores explícitos de dominio de aplicación (`ScoringDatasetNotFoundError`, `ScoringValidationError`) para separar fallos de acceso a datos y validación.
  - Se añadieron tests unitarios dedicados para predictor y use case usando fakes simples y sin dependencias externas.
- **Decisiones**:
  - El baseline se mantuvo 100% determinista y sin estado para permitir reproducibilidad, trazabilidad y debugging temprano sin pipeline de entrenamiento.
  - Se ubicó el predictor en `application` (no infraestructura) porque encapsula reglas puras de negocio/heurística.
  - Se priorizó contrato estable (`PredictorPort`) para poder reemplazar baseline por un modelo real sin romper casos de uso.
- **Riesgos**:
  - Las reglas heurísticas no sustituyen evaluación estadística real; pueden sesgar decisiones en distribución cambiante.
  - La deserialización actual asume schema de features estable; cambios de contrato requerirán versionado explícito.
- **Qué queda**:
  - Integrar modelo entrenable real (offline + evaluación), con métricas y calibración.
  - Definir pipeline de entrenamiento y registro de artefactos/versiones de modelo.
  - Añadir monitoreo de drift de datos/features y validaciones de scoring online.

- **DATE/TIME**: 2026-02-28 00:40 UTC
- **Paso**: Paso 8: Lineage + metadata + schema + hashes
- **Qué se hizo**:
  - Se creó el módulo de artefactos ML en application con modelos `FeatureSchemaField`, `FeatureSchema`, `FeatureArtifactMetadata` y helpers puros para hashing/canonicalización.
  - Se extendió el puerto `FeatureStorePort` con `save_with_metadata(...)` y `load_metadata(...)` manteniendo `save/load/list_versions` para compatibilidad.
  - Se evolucionó `LocalJsonFeatureStore` para persistir por versión tres artefactos (`.json`, `.metadata.json`, `.schema.json`) y para exponer errores explícitos cuando faltan metadata o schema.
  - Se añadió `FeatureStoreService.save_citas_features_with_artifacts(...)` para construir schema/hashes/metadata+quality report de `citas_features` y persistirlos de forma reproducible.
  - Se integró validación de metadata en `ScoreCitas`: cuando hay metadata valida `row_count` y `content_hash`; si no existe metadata continúa por compatibilidad agregando reason informativa.
  - Se añadió test suite `tests/test_feature_store_artifacts.py` con casos de creación de artefactos, hashes deterministas, estabilidad de schema y validación en scoring con metadata inconsistente.
- **Decisiones**:
  - Compatibilidad hacia atrás: scoring no falla si falta metadata en versiones antiguas; solo agrega reason `metadata no disponible para esta versión`.
  - `schema_hash` y `content_hash` se calculan sobre JSON canónico (`sort_keys=True`, `separators` compactos) para reproducibilidad binaria.
  - El schema persistido en infraestructura se deriva de las filas serializadas y el schema de aplicación se deriva del dataclass de features para contrato estable de `citas_features`.
- **Riesgos**:
  - No hay locking transaccional ni escritura atómica multiarchivo entre rows/schema/metadata.
  - Falta firma criptográfica de artefactos y registro global de datasets/versiones.
- **Qué queda**:
  - Añadir lock/atomic writes y estrategia anti-corrupción de artefactos parcialmente escritos.
  - Incorporar firma de artefactos y dataset registry centralizado con políticas de retención.
  - Definir validación de compatibilidad de `schema_version` durante scoring y entrenamiento.

- **DATE/TIME**: 2026-02-28 01:25 UTC
- **Paso**: Paso 9: Train + evaluate + model store offline
- **Qué se hizo**:
  - Se agregó `derive_target_from_feature(...)` como etiqueta proxy determinista (`1` si `has_incidencias` o `is_suspicious`, si no `0`) para habilitar pipeline end-to-end reproducible sin dependencias externas.
  - Se implementó modelo entrenable determinista tipo Naive Bayes discreto en application (`train/predict_one/predict_batch`) con suavizado de Laplace y payload serializable.
  - Se incorporó módulo de evaluación offline (`EvalMetrics`) con accuracy, precision, recall y matriz de confusión (`tp/fp/tn/fn`) usando threshold fijo `score>=0.5`.
  - Se definió el puerto `ModelStorePort` y su implementación `LocalJsonModelStore` para versionado offline en `base/models/<model>/<version>.*.json`.
  - Se creó caso de uso `TrainCitasModel` para: cargar features versionadas + metadata, validar `schema_hash`, entrenar, evaluar sobre el dataset actual, y registrar payload+metadata+métricas del modelo.
  - Se extendió `ScoreCitas` para seleccionar predictor por request (`baseline|trained`), cargar modelo entrenado por versión, validar compatibilidad de `schema_hash` y anexar reason `trained_model:<name>@<version>`.
  - Se añadieron tests core de entrenamiento, model store y scoring con modelo entrenado (incluyendo mismatch de schema).
- **Decisiones**:
  - Se eligió Naive Bayes discreto por ser corto, determinista, explicable y sin optimización numérica compleja.
  - Se conservó `baseline` como default para compatibilidad total hacia atrás.
  - La etiqueta se documenta explícitamente como **proxy label** (no ground-truth clínica).
- **Riesgos**:
  - La evaluación se ejecuta sobre el mismo dataset de entrenamiento (optimista y no generalizable).
  - El target proxy introduce leakage intencional en esta fase inicial; útil para validar plumbing, no para performance real.
  - Persistencia del model store es local JSON sin locking/atomicidad multiarchivo.
- **Qué queda**:
  - Incorporar split reproducible train/validation/holdout y/o CV.
  - Diseñar etiquetado real de negocio (ground truth) y calibración de scores.
  - Añadir validaciones de drift y políticas de promoción de modelos.

- **DATE/TIME**: 2026-02-28 02:10 UTC
- **Paso**: Paso 10: Evaluación holdout temporal + backtesting simple
- **Qué se hizo**:
  - Se añadió `application/ml/splitting.py` con `TemporalSplitConfig`, `temporal_train_test_split(...)`, error explícito `TemporalSplitNotEnoughDataError` y `temporal_folds(...)` walk-forward determinista.
  - Se evolucionó `CitasFeatureRow` incorporando `inicio_ts` (epoch int) para habilitar split temporal serializable sin romper persistencia JSON del feature store.
  - Se actualizó `TrainCitasModel` para cargar features, aplicar split temporal determinista (80/20, `min_train=20`), entrenar solo con train y evaluar por separado en train/test.
  - Se cambió metadata del modelo para incluir `split_config`, `train_metrics`, `test_metrics` y `test_row_count`; además el response del use case ahora expone ambas métricas.
  - Se agregó adaptación de error de split insuficiente a `TrainCitasModelNotEnoughDataError` para cortar entrenamiento de forma explícita.
  - Se añadieron tests core: `tests/test_temporal_split.py` y extensión de `tests/test_ml_training.py` para verificar split determinista, error por datos insuficientes y persistencia de métricas train/test.
- **Decisiones**:
  - Campo temporal elegido: `inicio_ts` en `CitasFeatureRow` para evitar serialización de `datetime` en artefactos JSON.
  - Configuración base de split: `test_ratio=0.2`, `min_train=20`, orden ascendente por tiempo.
  - Backtesting implementado como helper opcional en módulo de split con ventanas fijas 60/20, 80/20 y 90/10.
- **Limitaciones**:
  - `temporal_folds(...)` queda disponible pero no se integra aún en metadata de entrenamiento para mantener cambio mínimo del caso de uso.
  - El target sigue siendo proxy label derivada de features, útil para plumbing/CI pero no equivalente a ground-truth clínico.

- **DATE/TIME**: 2026-02-28 03:05 UTC
- **Paso**: Paso 11: Calibración de threshold + drift report
- **Qué se hizo**:
  - Se añadió calibración de threshold en `application/ml/calibration.py` con políticas `min_recall`, `min_precision` y `f1_max`, además de cálculo puro de métricas por threshold.
  - Se integró la calibración en `TrainCitasModel`: scoring en holdout temporal, selección de threshold calibrado y persistencia de `calibrated_threshold`, `calibration_policy` y `test_metrics_at_calibrated_threshold` en metadata del modelo.
  - Se actualizó `ScoreCitas` para predictor entrenado: aplica `calibrated_threshold` de metadata para etiqueta binaria (`risk`/`no_risk`) y agrega reason `threshold:<thr>`; mantiene fallback a `0.5` por compatibilidad.
  - Se añadió `application/ml/drift.py` con `DriftReport`, distribución categórica, PSI y cálculo de drift sobre features discretas clave.
  - Se añadió el caso de uso `DriftCitasFeatures` para comparar dos versiones de features vía `FeatureStoreService` y devolver `DriftReport`.
  - Se agregaron tests core dedicados para calibración, integración train+scoring calibrado y drift.
- **Decisiones**:
  - Policy por defecto de calibración: `objective=min_recall`, `value=0.80`, `threshold` fallback de `0.5`.
  - Umbral de alerta PSI: `0.2` para `overall_flag=True` cuando al menos una feature supera ese valor.
  - En objetivos `min_*`, si no hay threshold que cumpla target se devuelve el threshold más cercano por métrica objetivo.
- **Limitaciones**:
  - La calibración usa el test set temporal del mismo pipeline offline y etiqueta proxy; no sustituye validación clínica con ground-truth.
  - El reporte de drift actual cubre solo features categóricas discretas y no persiste artefacto JSON en store (solo retorno en use case).
- **Qué queda**:
  - Incorporar persistencia opcional del drift report como artefacto versionado y alarmas automáticas de CI/CD.
  - Evaluar thresholds dinámicos por segmento operativo (especialidad/sede) cuando existan labels reales.

- **DATE/TIME**: 2026-02-27 00:00 UTC
- **Paso**: Paso 12: CLI ML operativa
- **Qué se hizo**:
  - Se creó `scripts/ml_cli.py` con `argparse` (stdlib) y subcomandos `build-features`, `train`, `score` y `drift`, manteniendo el script como capa de orquestación sin lógica de negocio de dominio.
  - `build-features` ejecuta pipeline completo (`BuildCitasDataset` -> `build_citas_features` -> `compute_citas_quality_report` -> `FeatureStoreService.save_citas_features_with_artifacts`) y reporta `saved_version`, `row_count`, `suspicious_count`.
  - Se incorporó wiring de infraestructura para modo real vía SQLite (`bootstrap_database` + `SqliteCitasReadAdapter`) y fallback explícito `--demo-fake` con dataset determinista sintético (40 citas).
  - `train` orquesta `TrainCitasModel` con `LocalJsonFeatureStore` + `LocalJsonModelStore`; imprime versión y métricas principales con threshold calibrado.
  - `score` orquesta `ScoreCitas` baseline/trained, imprime tabla estable (`cita_id | score | label | reasons`) y resumen agregado por labels.
  - `drift` orquesta `DriftCitasFeatures` e imprime `psi_by_feature` + `overall_flag`.
  - Se añadió `tests/test_ml_cli_smoke.py` para smoke determinista llamando `main()` directamente (sin subprocess) sobre build/train/score/drift en `tmp_path`.
- **Decisiones de wiring**:
  - Se mantiene la CLI aislada en `scripts/` y solo instancia adaptadores concretos (`LocalJsonFeatureStore`, `LocalJsonModelStore`, `SqliteCitasReadAdapter`).
  - Se validó `model-name` contra el contrato actual (`citas_nb_v1`) para evitar rutas ambiguas mientras el use case de entrenamiento mantiene nombre fijo.
- **Modo demo-fake**:
  - Se añadió `--demo-fake` como fallback seguro y reproducible cuando no se use/disponga SQLite real.
  - Se añadió `--demo-profile baseline|shifted` para generar dos distribuciones distintas y habilitar demos de drift en segundos.

- **DATE/TIME**: 2026-02-28 03:45 UTC
- **Paso**: Paso 13: Capa de exportación CSV estable + CLI `export`
- **Qué se hizo**:
  - Se añadió `clinicdesk/app/application/usecases/export_csv.py` con casos de uso de exportación determinista (`ExportFeaturesCSV`, `ExportModelMetricsCSV`, `ExportScoringCSV`, `ExportDriftCSV`) usando solo stdlib (`csv`, `pathlib`).
  - Se definieron contratos de columnas estables y orden fijo para `features_export.csv`, `model_metrics_export.csv`, `scoring_export.csv` y `drift_export.csv`.
  - Se integró la CLI con comando `export` y subcomandos `features`, `metrics`, `scoring`, `drift`, manteniendo scripts como orquestación sin lógica de negocio.
  - Se añadió smoke de CLI y pruebas unitarias de export en `tests/test_csv_exports.py` usando `tmp_path` (sin tocar filesystem real).
  - Se documentó integración Power BI en `docs/ci_quality_gate.md` con comandos listos para copy/paste.
- **Decisiones**:
  - CSV con serialización determinista (`float` con 6 decimales, `bool` como `0/1`) para estabilidad de ingestion.
  - Para `export metrics`, la CLI adapta metadata del model store a DTO de export sin recalcular métricas.
- **Limitaciones**:
  - `dataset_version` en `export metrics` se recibe por CLI; no se infiere automáticamente desde metadata para mantener contrato explícito del comando.

- **DATE/TIME**: 2026-02-28 05:10 UTC
- **Paso**: Paso 14: Demo dataset seed reproducible (médicos/pacientes/citas)
- **Qué se hizo**:
  - Se creó módulo puro `application/demo_data` con DTOs desacoplados de SQLite y generadores deterministas para médicos, pacientes, personal, citas e incidencias.
  - Se añadió seeder de infraestructura `infrastructure/sqlite/demo_data_seeder.py` para persistir entidades con repos canónicos respetando FK y creando salas por defecto si faltan.
  - Se añadió caso de uso `SeedDemoData` con request/response tipadas para orquestar generación + persistencia.
  - Se extendió `scripts/ml_cli.py` con comando `seed-demo` y parámetros de configuración (`seed`, volúmenes, rango de fechas, incidencia y `--sqlite-path`).
  - Se añadieron tests puros del generador y test de integración SQLite del seeder.
  - Se documentó el flujo end-to-end para demos (seed → build-features → train → export → Power BI).
- **Decisiones**:
  - Mantener el script CLI como orquestador, delegando reglas de negocio de generación al módulo puro de application.
  - Usar `random.Random(seed + offset)` por entidad para preservar reproducibilidad y permitir crecimiento modular.
  - Incluir distribución realista base: mayoría laborables, buckets de duración y outliers controlados.
- **Riesgos**:
  - El comando `seed-demo` inserta incrementalmente (no limpia tablas); ejecuciones repetidas sobre la misma BD pueden acumular datos demo.
- **Qué queda**:
  - Evaluar comando opcional de limpieza/reset para escenarios de demo repetibles en la misma base persistente.
